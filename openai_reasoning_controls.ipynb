{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc87b838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "# Ensure your API key is set in your environment\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"Client initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b065ad11",
   "metadata": {},
   "source": [
    "## The Test Problem\n",
    "We will use a Fermi estimation problem that benefits from deeper reasoning but also needs a clean final answer.\n",
    "\"How many piano tuners are there in Chicago?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c93f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A slightly complex reasoning task\n",
    "prompt = \"Estimate the number of piano tuners in Chicago based on first principles. Use Fermi estimation steps.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a87ff97",
   "metadata": {},
   "source": [
    "## 1. Deliberation Control: `reasoning.effort`\n",
    "**Goal:** Control how much \"thinking\" the model does before answering.\n",
    "\n",
    "**Parameters:** `reasoning={\"effort\": \"low\" | \"medium\" | \"high\"}`\n",
    "\n",
    "We will compare \"low\" effort vs \"high\" effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b5b4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function defined.\n"
     ]
    }
   ],
   "source": [
    "# Helper function to print results clearly\n",
    "def print_output(title, response):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {title}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Reasoning Tokens Used: {response.usage.output_tokens_details.reasoning_tokens}\")\n",
    "    print(f\"Total Output Tokens: {response.usage.output_tokens}\")\n",
    "    print(f\"\\nFull Answer:\")\n",
    "    print(\"-\"*60)\n",
    "    print(response.output_text)\n",
    "    print(\"-\"*60)\n",
    "\n",
    "print(\"Helper function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f55103",
   "metadata": {},
   "source": [
    "### 1A. Low Reasoning Effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068513a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting Low Effort...\n",
      "\n",
      "============================================================\n",
      "  Low Effort Result\n",
      "============================================================\n",
      "Reasoning Tokens Used: 0\n",
      "Total Output Tokens: 704\n",
      "\n",
      "Full Answer:\n",
      "------------------------------------------------------------\n",
      "### Goal\n",
      "Estimate the number of **piano tuners in Chicago** using a Fermi (first-principles) approach.\n",
      "\n",
      "---\n",
      "\n",
      "## 1) Estimate Chicago population and households\n",
      "- Chicago city population ≈ **2.7 million**\n",
      "- Average household size ≈ **2.5 people/household**\n",
      "- Number of households ≈ \\( 2.7\\text{M} / 2.5 \\approx 1.1\\text{M} \\) households\n",
      "\n",
      "---\n",
      "\n",
      "## 2) How many households have a piano?\n",
      "Piano ownership in US households is often on the order of a few percent (higher in affluent/older households, lower elsewhere). Use a plausible range:\n",
      "\n",
      "- Piano-owning households fraction ≈ **2%–5%**\n",
      "- Piano households in Chicago ≈ \\( 1.1\\text{M} \\times (0.02\\text{ to }0.05) \\approx 22{,}000\\text{ to }55{,}000 \\)\n",
      "\n",
      "Add non-household pianos (schools, churches, venues, studios):\n",
      "- As a rough uplift: **+20%** to household count\n",
      "\n",
      "Total pianos needing service ≈ \\( (22k\\text{ to }55k) \\times 1.2 \\approx 26k\\text{ to }66k \\)\n",
      "\n",
      "---\n",
      "\n",
      "## 3) How often does a piano get tuned?\n",
      "Not every piano gets tuned annually; some are neglected, while institutions tune more often.\n",
      "\n",
      "Assume average tuning frequency across all pianos:\n",
      "- **0.7 tunings per piano per year** (some 0, some 1–2+)\n",
      "\n",
      "Annual tuning jobs ≈ \\( (26k\\text{ to }66k) \\times 0.7 \\approx 18k\\text{ to }46k \\) tunings/year\n",
      "\n",
      "---\n",
      "\n",
      "## 4) How many tunings can one tuner do per year?\n",
      "Typical workload:\n",
      "- Tunings per day: **~2–3** (travel + job time)\n",
      "- Working days per year: **~220** (allowing weekends, holidays, admin, seasonal variability)\n",
      "\n",
      "Annual capacity per tuner:\n",
      "- \\( 2.5 \\times 220 \\approx 550 \\) tunings/year (reasonable midpoint)\n",
      "- Plausible range: **400–700** tunings/year\n",
      "\n",
      "---\n",
      "\n",
      "## 5) Convert annual demand into number of tuners\n",
      "Using midpoint capacity ~550 tunings/year:\n",
      "\n",
      "- Low estimate: \\( 18{,}000 / 550 \\approx 33 \\)\n",
      "- High estimate: \\( 46{,}000 / 550 \\approx 84 \\)\n",
      "\n",
      "So a reasonable Fermi estimate is:\n",
      "\n",
      "### **~30 to 90 piano tuners in Chicago**\n",
      "A single-point “best guess” (midrange) would be around:\n",
      "### **~60 piano tuners**\n",
      "\n",
      "---\n",
      "\n",
      "## Quick sanity check\n",
      "If there are ~60 tuners and each does ~550 tunings/year, that’s:\n",
      "- \\( 60 \\times 550 \\approx 33{,}000 \\) tunings/year  \n",
      "Which corresponds to ~47,000 pianos tuned at 0.7/year—consistent with the earlier estimate band.\n",
      "\n",
      "---\n",
      "\n",
      "**Answer:** **On the order of ~10¹–10²**, likely **~60** piano tuners in Chicago (roughly **30–90**).\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1A. Low Effort\n",
    "print(\"Requesting Low Effort...\")\n",
    "response_low = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    reasoning={\"effort\": \"low\"}\n",
    ")\n",
    "print_output(\"Low Effort Result\", response_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e4eb2b",
   "metadata": {},
   "source": [
    "### 1B. High Reasoning Effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe832da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting High Effort...\n",
      "\n",
      "============================================================\n",
      "  High Effort Result\n",
      "============================================================\n",
      "Reasoning Tokens Used: 417\n",
      "Total Output Tokens: 1118\n",
      "\n",
      "Full Answer:\n",
      "------------------------------------------------------------\n",
      "### Goal\n",
      "Estimate the number of **piano tuners in Chicago (city proper)** using a Fermi (“back of the envelope”) approach.\n",
      "\n",
      "---\n",
      "\n",
      "## 1) How many pianos are in Chicago?\n",
      "\n",
      "**Chicago population:** ~2.7 million  \n",
      "**Average household size:** ~2.5 people/household  \n",
      "\\[\n",
      "\\text{Households} \\approx \\frac{2.7\\text{M}}{2.5} \\approx 1.1\\text{ million households}\n",
      "\\]\n",
      "\n",
      "**Fraction of households with a piano:** assume ~5% (≈ 1 in 20)  \n",
      "\\[\n",
      "\\text{Household pianos} \\approx 1.1\\text{M} \\times 0.05 \\approx 55{,}000\n",
      "\\]\n",
      "\n",
      "Add **non-household pianos** (schools, churches, venues, universities, rehearsal spaces). A simple way: add ~15–25% extra. Use **~20%**:\n",
      "\\[\n",
      "\\text{Total pianos} \\approx 55{,}000 \\times 1.2 \\approx 66{,}000\n",
      "\\]\n",
      "\n",
      "So, **~70k pianos** is a reasonable round figure.\n",
      "\n",
      "---\n",
      "\n",
      "## 2) How many piano tunings happen per year?\n",
      "\n",
      "**Average tunings per piano per year:** many are neglected, some are tuned 2–4×/year. A blended average of **~1 tuning/piano/year** is a common Fermi assumption.\n",
      "\\[\n",
      "\\text{Annual tunings} \\approx 70{,}000 \\times 1 \\approx 70{,}000 \\text{ tunings/year}\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "## 3) How many tunings can one tuner do per year?\n",
      "\n",
      "Estimate a tuner’s throughput:\n",
      "\n",
      "- Tunings per day: assume **~2 tunings/day** (time on-site + travel + scheduling + occasional repairs)\n",
      "- Working days/year: ~250 (≈ 5 days/week × 50 weeks)\n",
      "\n",
      "\\[\n",
      "\\text{Tunings per tuner per year} \\approx 2 \\times 250 = 500\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "## 4) Number of tuners\n",
      "\n",
      "\\[\n",
      "\\text{Tuners} \\approx \\frac{70{,}000}{500} \\approx 140\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "# Estimate\n",
      "**≈ 140 piano tuners in Chicago** (order of magnitude: **~100–200**)\n",
      "\n",
      "### Quick sensitivity check (to show robustness)\n",
      "- If only **3%** of households have pianos and average tuning is **0.8/year**, you might get:  \n",
      "  pianos ≈ 1.1M×0.03×1.2 ≈ 40k → tunings ≈ 32k → tuners ≈ 60–80\n",
      "- If **7%** have pianos and average tuning is **1.2/year**, you might get:  \n",
      "  pianos ≈ 1.1M×0.07×1.2 ≈ 92k → tunings ≈ 110k → tuners ≈ 150–250\n",
      "\n",
      "So the Fermi answer stays in the **low hundreds**, centered around **~140**.\n",
      "\n",
      "If you want, I can redo the estimate for the **Chicago metro area** instead of the city proper (it will be several times larger).\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1B. High Effort\n",
    "print(\"Requesting High Effort...\")\n",
    "response_high = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    reasoning={\"effort\": \"high\"}\n",
    ")\n",
    "print_output(\"High Effort Result\", response_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf3406",
   "metadata": {},
   "source": [
    "## 2. Stopping Controls: `max_output_tokens`\n",
    "**Goal:** Prevent runaway costs or \"overthinking\" by setting a hard budget on total output tokens.\n",
    "\n",
    "**Parameter:** `max_output_tokens=500` \n",
    "\n",
    "This caps the *total* output token budget (reasoning + visible answer). When paired with high effort, if the model spends too many tokens reasoning, the visible answer may be truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65640f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting High Effort with Output Budget (500 tokens)...\n",
      "\n",
      "============================================================\n",
      "  Budget Constrained Result\n",
      "============================================================\n",
      "Reasoning Tokens Used: 500\n",
      "Total Output Tokens: 500\n",
      "\n",
      "Full Answer:\n",
      "------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "[INFO] Budget: 500 tokens\n",
      "  - Reasoning tokens: 500\n",
      "  - Visible answer tokens: 0\n",
      "  - Total output: 500\n",
      "[!] Output may have been truncated due to budget constraint.\n"
     ]
    }
   ],
   "source": [
    "# 2. Strict Output Budget\n",
    "# We set a total output token limit (reasoning + visible answer)\n",
    "output_budget = 500\n",
    "\n",
    "print(f\"Requesting High Effort with Output Budget ({output_budget} tokens)...\")\n",
    "\n",
    "response_budget = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    reasoning={\"effort\": \"high\"},\n",
    "    max_output_tokens=output_budget\n",
    ")\n",
    "\n",
    "print_output(\"Budget Constrained Result\", response_budget)\n",
    "\n",
    "# Check token usage\n",
    "reasoning_used = response_budget.usage.output_tokens_details.reasoning_tokens\n",
    "total_output = response_budget.usage.output_tokens\n",
    "visible_tokens = total_output - reasoning_used\n",
    "\n",
    "print(f\"\\n[INFO] Budget: {output_budget} tokens\")\n",
    "print(f\"  - Reasoning tokens: {reasoning_used}\")\n",
    "print(f\"  - Visible answer tokens: {visible_tokens}\")\n",
    "print(f\"  - Total output: {total_output}\")\n",
    "\n",
    "if total_output >= output_budget:\n",
    "    print(\"[!] Output may have been truncated due to budget constraint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7c498",
   "metadata": {},
   "source": [
    "## 3. Output Shaping via Prompt Engineering\n",
    "**Goal:** Control what the user actually sees, regardless of the internal reasoning depth.\n",
    "\n",
    "**Technique:** Since the Responses API doesn't support `response_format`, we use explicit prompt instructions to shape the output into JSON.\n",
    "\n",
    "Even if the model thinks for thousands of tokens, we can ask it to deliver a clean JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9b4fb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requesting High Effort with Output Shaping (JSON via Prompt)...\n",
      "\n",
      "--- Shaped Output (High Reasoning, JSON View) ---\n",
      "Reasoning Tokens (Hidden): 747\n",
      "Visible Answer:\n",
      "{\n",
      "  \"population_chicago\": 2700000,\n",
      "  \"households_with_pianos\": 6.0,\n",
      "  \"tuning_frequency_per_year\": 1.0,\n",
      "  \"total_tunings_needed\": 77625,\n",
      "  \"tunings_per_tuner_per_year\": 600,\n",
      "  \"estimated_tuners\": 129\n",
      "}\n",
      "\n",
      "[SUCCESS] Output is valid JSON.\n"
     ]
    }
   ],
   "source": [
    "# 3. Structured Output (via Prompt Engineering)\n",
    "# Since the 'response_format' parameter is not currently supported by the client.responses.create method \n",
    "# in this environment, we will use explicit prompt instructions to shape the output into JSON.\n",
    "\n",
    "json_instruction = \"\"\"\n",
    "Output the final answer as a valid JSON object with the following keys:\n",
    "- population_chicago (integer)\n",
    "- households_with_pianos (number, decimal % estimate)\n",
    "- tuning_frequency_per_year (number)\n",
    "- total_tunings_needed (integer)\n",
    "- tunings_per_tuner_per_year (integer)\n",
    "- estimated_tuners (integer)\n",
    "\n",
    "Do not include markdown formatting (like ```json). Just the raw JSON string.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nRequesting High Effort with Output Shaping (JSON via Prompt)...\")\n",
    "\n",
    "# Append instructions to the user prompt\n",
    "shaped_prompt = prompt + \"\\n\\n\" + json_instruction\n",
    "\n",
    "response_shaped = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    input=[{\"role\": \"user\", \"content\": shaped_prompt}],\n",
    "    reasoning={\"effort\": \"high\"}\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Shaped Output (High Reasoning, JSON View) ---\")\n",
    "print(f\"Reasoning Tokens (Hidden): {response_shaped.usage.output_tokens_details.reasoning_tokens}\")\n",
    "print(f\"Visible Answer:\\n{response_shaped.output_text}\")\n",
    "\n",
    "# Verify it parses as JSON\n",
    "try:\n",
    "    data = json.loads(response_shaped.output_text)\n",
    "    print(\"\\n[SUCCESS] Output is valid JSON.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"\\n[WARNING] Output is not valid JSON (Prompt shaping is less strict than response_format).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning-models-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
